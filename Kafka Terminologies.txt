> CLUSTER: 

- The Kafka cluster is a combination of multiple Kafka nodes. 
- On top of Kafka nodes, we need to deploy the multiple Kafka services like Kafka Broker, Kafka consumer, Kafka Producer, Zookeeper, etc.
- A Kafka cluster is a system that consists of several Brokers, Topics, and Partitions for both. 
- The key objective is to distribute workloads equally among replicas and Partitions.

> BROKER:

- A Kafka broker allows consumers to fetch messages by topic, partition and offset. 
- Kafka brokers can create a Kafka cluster by sharing information between each other directly or indirectly using Zookeeper. 
- A Kafka cluster has exactly one broker that acts as the Controller.

> TOPICS: 

- A Kafka Topic is essentially a category to help in organizing and managing messages. 
- A Kafka Topic will have a unique name within the Kafka cluster. 
- Messages are sent to and read from individual Topics. 
- The Kafka Producers writes data to a Topic and the Kafka Consumer reads the data from a Topic.
- Every Kafka Topic is partitioned and replicated across different Kafka Brokers (Kafka servers or nodes in a Kafka Cluster). 
- This distributed placement enables client applications to read and write data to and from multiple brokers simultaneously.
- When a new event is written to a Kafka Topic, it is attached to a Topic partition. 
- Events that involve the same event key such as a specific customer id or payment id will be written to the same partition always, and the Consumer of that topic-partition will always get to read the events in the same sequence as they were written. 
- Partitions are integral to Kafka’s functioning since they enable topic parallelization and high message throughput.

> RECORDS: 

- Records are essentially information about events that are stored on the Topic as a record. 
- Applications can connect and transfer a record into the Topic. 
- Data is durable and can be stored in a topic for a long time till the specified retention period is over.
- These records on a topic can be processed and reprocessed by applications that are connected to the Kafka system.
- An individual record will have 2 mandatory attributes -the Key and the Value and 2 optional attributes -the Timestamp and the Header.

> ZOOKEEPER:

- Apache Zookeeper is a software that monitors and maintains order within a Kafka system and behaves as a centralized, distributed coordination service for the Kafka Cluster. 
- It maintains data about configuration and naming and is responsible for synchronization within all the distributed systems. 
- Apache Zookeeper tracks Kafka cluster node statuses, Kafka messages, partitions, and topics among other things.
- Apache Zookeeper co-ordinates the Kafka Cluster. Kafka needs Zookeeper to be installed before it can be used in production. 
- This is essential even if the system consists of a single broker, topic, and partition. 
- Basically, Zookeeper has five use cases – electing a controller, cluster membership, configuration of topics, access control lists (ACLs) and monitoring quotas.

> PRODUCER & CONSUMER:

- Kafka Producers are basically client applications that publish or write events to Kafka.
- Consumers are those systems that subscribe to or read and process those events.
- Kafka Producers and Kafka Consumers are completely decoupled and there are no dependencies between them. 
- This is one of the reasons Apache Kafka is so highly scalable. 
- The capability to process events exactly-once is one of the guarantees that comes with Kafka.
- Kafka producers & consumers are basic client APIs through which we can interact with kafka. 

> KAFKA CONNECT:

- Kafka Connect is the framework that connects databases, search indexes, file systems and key-value stores to Kafka with ready-to-use components called Kafka Connectors. 
- Kafka connectors can deliver data from external systems to Kafka topics and deliver data from Kafka topics to external systems.
- Kafka connectors are of two types: 

1. Source Connectors - A Kafka Source Connector aggregates data from source systems like databases, streams, or message brokers. A source connector can also collect metrics from application servers into Kafka topics for stream processing in near real-time.

2. Sink Connectors - A Kafka Sink Connector exports data from Kafka topics into other systems. These could be popular databases like Oracle, SQL Server, SAP, or indexes such as Elasticsearch, batch systems like Hadoop, cloud platforms like Snowflake, Amazon S3, Redshift, Azure Synapse etc.

> KAFKA STREAMS: 

- Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in an Apache Kafka cluster. 
- It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka's server-side cluster technology.